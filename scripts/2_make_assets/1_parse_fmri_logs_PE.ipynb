{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The purpose of this script is to parse output from PsychoPy into fMRI log files (event timing files for each condition / stimulus, which can later be fed to FEAT). You must run this script individually for each participant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import necessaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pylab inline\n",
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:80% !important; }</style>\"))\n",
    "\n",
    "pd.set_option('display.max_rows', 150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define top dir\n",
    "topDir = open('../top_dir_win.txt').read().replace('\\n', '')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Specify subject and input data\n",
    "subj = \"subject-001\" \n",
    "\n",
    "inFilePath = topDir + \"/behavioural_data/fmri_runs1/\" + subj + '/'\n",
    "\n",
    "# # Specify directories for output (and make directories if they don't alraedy exist)\n",
    "outFilePath = topDir + \"/MRIanalyses/\" + 'assets/' + subj + '/' + subj + \"_log_files/\"\n",
    "if not os.path.exists(outFilePath):\n",
    "    os.makedirs(outFilePath)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load behavioural data from each run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define columns we want to read in\n",
    "studyCols = ['cond','studied_word', 'studyWord.started']\n",
    "testCols = ['test_cond', 'tested_word', 'testWord.started', 'test_resp.corr']           \n",
    "\n",
    "# The easiest way to deal with multiple datasets is to define a function for reading in the data\n",
    "def readData(run):\n",
    "    \n",
    "    # Look for inFile for study1 in 'true first run' folder, for subject-014 only (see behavioural_data/data_collection_notes.txt for explanation)\n",
    "    if subj=='subject-014' and run=='study1':\n",
    "        inFile = inFilePath + 'true-first-run/subject-014_study2.csv'  # This might look incorrect, but it's not - see data collection notes!\n",
    "    \n",
    "    # Same issue with subject-016\n",
    "    elif subj=='subject-016' and run=='study1':\n",
    "        inFile = inFilePath + 'true-first-run/subject-016_study2.csv' \n",
    "    \n",
    "    # Had some trouble getting started with subject-018, which led to multiple csv's for study1. See data collection notes\n",
    "    elif subj=='subject-018' and run=='study1':\n",
    "        inFile = inFilePath + 'subject-018_study1_3.csv'\n",
    "        \n",
    "    # For some reason subject-022's study1 data is labelled study1_1. Must have re-started experiment during setup.\n",
    "    if subj=='subject-022' and run=='study1':\n",
    "        inFile = inFilePath + 'subject-022_study1_1.csv' \n",
    "        \n",
    "    else:\n",
    "        inFile = inFilePath + subj + \"_\" + run + \".csv\"\n",
    "    \n",
    "    if run == 'study1' or run == 'study2':\n",
    "        columns = studyCols\n",
    "    elif run == 'test':\n",
    "        columns = testCols\n",
    "        \n",
    "    df = pd.read_csv(inFile, usecols = columns)\n",
    "\n",
    "    \n",
    "    # Remoave NaNs\n",
    "    df.dropna(inplace=True)\n",
    "    \n",
    "    # Rename columns for convenience \n",
    "    df.rename(columns={'cond':'CONDITION', 'studied_word':'WORD', 'studyWord.started': 'WORDON',\n",
    "                      'test_cond':\"CONDITION\", 'tested_word':'WORD', 'testWord.started':'WORDON',\n",
    "                      'test_resp.corr': 'ACC'}, inplace=True)\n",
    "    \n",
    "    # Add run #\n",
    "    df['RUN'] = run\n",
    "    \n",
    "    return df\n",
    "\n",
    "df_study1 = readData('study1') # study 1\n",
    "df_study2 = readData('study2') # study 2\n",
    "df_test = readData('test') # test phase (does not have a run # because there's only one run!)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a placeholder ACC column to study phase data (hack so we can later use WORD as a key for filling in responses)\n",
    "df_study1['ACC'] = df_study1['WORD']\n",
    "df_study2['ACC'] = df_study2['WORD']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make dict mapping words to their test phase responses\n",
    "testResps = pd.Series(df_test.ACC.values,index=df_test.WORD).to_dict()\n",
    "# now replace values in Study phase w responses\n",
    "df_study1.replace({'ACC':testResps}, inplace=True)\n",
    "df_study2.replace({'ACC':testResps}, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Adjust stimulus timing\n",
    "\n",
    "Note that stim timings are relative to hitting \"start\" in PsychoPy, however there is a variable (uncontrolled) period between \"start\" and the actual onset of trails/scans, because the experiment begins with an instruction/\"please wait\" screen. Therefore, we need to make timings relative to the onset of trials, which was triggered by the MRI tech hitting \"s\" as they initiated the scan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to fix stim timing\n",
    "def correctTimings(run):\n",
    "\n",
    "    # Read in log file (which tells us exactly when the \"s\" was pressed, relative to PsychoPy starting)\n",
    "    \n",
    "    # As above, we have to look in a weird place for subject-014 study1\n",
    "    if subj=='subject-014' and run=='study1':\n",
    "        log_file = inFilePath + 'true-first-run/subject-014_study2.log'\n",
    "    elif subj=='subject-016' and run=='study1':\n",
    "        log_file = inFilePath + 'true-first-run/subject-016_study2.log'\n",
    "    else:    \n",
    "        log_file = inFilePath + subj + \"_\" + run + '.log'\n",
    "    \n",
    "    \n",
    "    # Read in as list (each line is an element)\n",
    "    f = open(log_file)\n",
    "    log = f.readlines()\n",
    "    \n",
    "    # Pull out the line containing \"Keypress: s\" (there should only be one instance in the whole run)\n",
    "    for i in log:\n",
    "        if \"Keypress: s\" in i:\n",
    "#             print(i)\n",
    "            startLine = i\n",
    "    \n",
    "    # Extract timing\n",
    "    absStart = float(startLine.split(\"\\t\")[0])\n",
    "    \n",
    "    # Call the df for this run\n",
    "    df = eval('df_' + run)\n",
    "    \n",
    "    # Subtract the absolute start time from WORDON. Also subtract the time for dummy scans (5 TRs, with each TR being 1.8 s)\n",
    "    df['WORDON_adj'] = df['WORDON'] - (absStart + (1.8 * 5))\n",
    "    \n",
    "    # Return the adjusted dataframe\n",
    "    return df\n",
    "            \n",
    "# Use the function to adjust stim timing in each df. Note that the first timepoint should always begin at \n",
    "# approximately 1.5 seconds, because the first cue (which ought to start at timepoint 0) lasts 1.5 seconds.\n",
    "df_study1_adj = correctTimings('study1')\n",
    "df_study2_adj = correctTimings('study2')\n",
    "df_test_adj = correctTimings('test')\n",
    "#dft_adj = correctTimings('3')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a look\n",
    "# df_test_adj"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate the df's from all three runs \n",
    "df = pd.concat([df_study1_adj, df_study2_adj, df_test_adj])\n",
    "\n",
    "# Add word duration\n",
    "df['wordDuration'] = 2.5\n",
    "\n",
    "# Reset index\n",
    "df.reset_index(inplace=True)\n",
    "df.drop(axis=1,labels='index', inplace=True)\n",
    "\n",
    "# df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Quality checking! \n",
    "Confirm that each word appears 4 times, and that each word is always in the same condition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm that df contains 120 rows: (60 x 2 for study phase, 90 for test)\n",
    "if len(df)==210:\n",
    "    print(\"210 rows in df - all good!\")\n",
    "else:\n",
    "    print(\"WARNING - there are\", len(df), \"rows in df!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm that every word occurs 3 times (no output unless there is a problem)\n",
    "for i in list(df['WORD']):\n",
    "    if list(df['WORD']).count(i)!=3:\n",
    "        \n",
    "        ## commented out because it generates a lot of useless output ##\n",
    "        \n",
    "        # Foil words will only occur once. So, check whether suspect words are foils\n",
    "#         if all(df.loc[df['WORD'] == i, 'CONDITION'] == 'foil'):\n",
    "#             print(i, \"appears\", list(df['WORD']).count(i), \"times, but it is a foil word\")\n",
    "\n",
    "\n",
    "\n",
    "        # If the word does not appear 3 times, and is NOT a foil word, print a warning\n",
    "        if all(df.loc[df['WORD'] == i, 'CONDITION'] != 'foil'): \n",
    "            print(\"WARNING WARNING\", i, \"appears\", list(df['WORD']).count(i), \"times WARNING WARNING\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Confirm that every word is always in the same condition. \n",
    "\n",
    "# To do this, we need to make a copy of df containing only WORD and CONDITION\n",
    "df_debug = df[['WORD','CONDITION']].copy()\n",
    "\n",
    "# Optional: Try artificially messing with word-condition mapping (by shuffling the condition column), to test whether this works \n",
    "# df_debug['CONDITION'] = np.random.permutation(df_debug['CONDITION'].values)\n",
    "\n",
    "aloud_debug = df_debug.loc[df_debug['CONDITION'] == 'aloud', 'WORD'].tolist()\n",
    "silent_debug = df_debug.loc[df_debug['CONDITION'] == 'silent', 'WORD'].tolist()\n",
    "foil_debug = df_debug.loc[df_debug['CONDITION'] == 'foil', 'WORD'].tolist()\n",
    "\n",
    "# Check for any overlap between the three lists. If there is no overlap, we are golden.\n",
    "\n",
    "# This is a bit messy but it works:\n",
    "as_overlap = list(set(aloud_debug) & set(silent_debug))\n",
    "af_overlap = list(set(aloud_debug) & set(foil_debug))\n",
    "fs_overlap = list(set(foil_debug) & set(silent_debug))\n",
    "\n",
    "overlap = as_overlap + af_overlap + fs_overlap\n",
    "\n",
    "if len(as_overlap)==0 or len(af_overlap)==0 or len(fs_overlap)==0:\n",
    "    print(\"Each word is always in the same condition\")\n",
    "else:\n",
    "    print(\"WARNING:\",len(overlap),\" word(s) appeared in multiple conditions. These are:\", overlap)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can also check manually, if desired\n",
    "# df.sort_values([\"WORD\",\"CONDITION\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate condition-wise output log files for FSL\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop through runs and conditions\n",
    "conditions = ['aloud','silent', 'foil']\n",
    "runs = ['study1', 'study2', 'test']\n",
    "\n",
    "for run in runs:\n",
    "    for cond in conditions:\n",
    "        dfTmp = df[df['CONDITION'].isin([cond]) & df['RUN'].isin([run])][['WORDON_adj','wordDuration']]\n",
    "        \n",
    "        # Skip empty intances (e.g. there are no foil words in study1 or study2)\n",
    "        if dfTmp.empty:\n",
    "            continue\n",
    "            \n",
    "        dfTmp['col3'] = 1\n",
    "        dfTmp.to_csv(outFilePath + '/' + subj + '_PE_' + str(run) + '_' + cond + '_alltrials.txt', sep='\\t', header=False, index=False) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generate trial-wise output log files for FSL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loop over the list of words, saving a separate log file for each word/run combination. \n",
    "wordList = list(set(df['WORD']))\n",
    "\n",
    "for word in wordList:\n",
    "    for cond in conditions:\n",
    "        for run in runs:\n",
    "            dfTmp = df[df['CONDITION'].isin([cond]) & df['RUN'].isin([run]) & df['WORD'].isin([word])][['WORDON_adj','wordDuration']]\n",
    "            \n",
    "            # Python will try to make a file for every possible combination of word, condition, and run. This means we get 2X the number of\n",
    "            # files we want, because it tries to match every word to both silent and aloud, resulting in one empty dfTmp for every word. To avoid\n",
    "            # this, skip every iteration where dfTmp is empty.\n",
    "            if dfTmp.empty:\n",
    "                continue\n",
    "            \n",
    "            dfTmp['col3'] = 1\n",
    "            dfTmp.to_csv(outFilePath + '/' + subj + '_PE_' + str(run) + '_' + cond + '_' + word + '.txt', sep='\\t', header=False, index=False) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# USe this for quality checking\n",
    "# df.loc[df['WORD']=='holiday']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Write lists for correct and incorrect words to file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take data from test phase, and only for aloud and silent words (makes things easier)\n",
    "dft = df[df['RUN'].isin(['test']) & df['CONDITION'].isin(['aloud','silent'])]\n",
    "\n",
    "corr_words = dft[dft['ACC']==1.0]['WORD'].tolist()\n",
    "incorr_words = dft[dft['ACC']==0.0]['WORD'].tolist()\n",
    "\n",
    "acc_words_path = \"../../MRIanalyses/\" + 'assets/' + subj + '/' \n",
    "\n",
    "# Make sure there is no overlap - if everything is okay, write to disk\n",
    "if len(list(set(corr_words) & set(incorr_words))) > 0:\n",
    "    print(\"WARNING: some words are categorized as both correct and incorrect\")\n",
    "else:\n",
    "    for thisList in ['corr_words', 'incorr_words']:\n",
    "        fn = acc_words_path + subj + '_PE_' + thisList + '.txt'\n",
    "        with open(fn, 'w') as f:\n",
    "            for item in eval(thisList):\n",
    "                f.write(\"%s\\n\" % item)\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernel_info": {
   "name": "python3"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "nteract": {
   "version": "0.8.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
